{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, date, timedelta\n",
    "import os\n",
    "import sqlalchemy as sa\n",
    "from sqlalchemy import create_engine, Date, text\n",
    "from config import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función que convierte un Json en un DataFrame\n",
    "def build_table(json_data):\n",
    "    df = pd.json_normalize(json_data)\n",
    "    return df\n",
    "\n",
    "def proceso_agregar_columnas(df):\n",
    "     # Convierte la columna 'timestamp_measured' en datetime si es de tipo objeto\n",
    "    df['fechaActualizacion'] = pd.to_datetime(df['fechaActualizacion'])\n",
    "    # Ahora puedes realizar las operaciones con las fechas\n",
    "    df[\"fecha\"] = df['fechaActualizacion'].dt.date\n",
    "    df[\"hora\"] = df['fechaActualizacion'].dt.hour\n",
    "\n",
    "    return df\n",
    "\n",
    "# Función que obtiene el dato y retorna una DataFrame\n",
    "def get_data(base_url, endpoint, params=None):\n",
    "   \n",
    "   try:  \n",
    "       # Apuntamos a la url del endpoit, hacemos la petición de tipo GET \n",
    "       # y la guardamos el objeto de tipo Response en la variable response\n",
    "         endpoint_url = f\"{base_url}/{endpoint}\"\n",
    "         response = requests.get(endpoint_url, params=params)\n",
    "         response.raise_for_status() \n",
    "         \n",
    "       #  LLamamos a la funcion def build_table(data) para convertir el dato en DataFrame y luego agregamos columnas \n",
    "         data=response.json()\n",
    "         df_data= build_table(data)\n",
    "         df_data= proceso_agregar_columnas(df_data)\n",
    "         \n",
    "         return df_data\n",
    "     \n",
    "   except requests.exceptions.RequestException as e:\n",
    "        print(f\"La petición ha fallado. Código de error : {e}\")\n",
    "        return None \n",
    "    \n",
    "\n",
    "def save(df, save_path, partition_cols=None, engine=\"fastparquet\"):\n",
    "    # Crear el directorio si no existe\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    \n",
    "    df.to_parquet(\n",
    "        save_path,\n",
    "        partition_cols=partition_cols,\n",
    "        engine=engine\n",
    "    )\n",
    "    \n",
    "    \n",
    "\n",
    "def connect_to_postgres():\n",
    "    \"\"\"\n",
    "    Establece una conexión a una base de datos postgres\n",
    "    utilizando la configuración especificada en un archivo INI.\n",
    "\n",
    "    \n",
    "    Returns:\n",
    "        sqlalchemy.engine.Engine: La conexión a la base de datos de postgres.\n",
    "    \"\"\"\n",
    "    # Comprobar si el archivo de configuración existe\n",
    "\n",
    "      # Establecer la conexión a la base de datos PostgreSQL\n",
    "    url = f\"postgresql://{user}:{pwd}@{host}:{port}/{db}?sslmode=require\"\n",
    "    conn = sa.create_engine(url,\n",
    "                              connect_args={\"options\": f\"-c search_path={schema}\"}\n",
    "                              )\n",
    "    return conn\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Extracción de API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL base y tres endpoints.\n",
    "base_url = \"https://dolarapi.com\"\n",
    "endpoint_dolar_oficial = \"/v1/dolares/oficial\" # Técnica extracción full\n",
    "endpoint_dolar_blue = \"/v1/dolares/blue\" # Técnica extracción incremental \n",
    "endpoint_dolares = \"/v1/dolares\" # Técnica extracción full\n",
    "\n",
    "# #DataFrame de todos los tipos de cambio  \n",
    "df_dolares= get_data(base_url, endpoint_dolares)\n",
    "df_dolar_oficial= get_data(base_url, endpoint_dolar_oficial)\n",
    "df_dolar_blue= get_data(base_url, endpoint_dolar_blue)\n",
    "\n",
    "\n",
    "dfs = [df_dolares, df_dolar_oficial, df_dolar_blue]\n",
    "paths = [\"tipos_cambio_dolar\", \"dolar_oficial\", \"dolar_blue\"]\n",
    "partition_cols_list = [\"fecha\", \"fecha\", \"fecha\"]\n",
    "\n",
    "# Itera sobre los endpoints, nombres de archivo y columnas de partición para obtener y guardar los datos en la capa bronze\n",
    "for df, path, partition_cols in zip(dfs, paths, partition_cols_list):\n",
    "      save(df, f'datalake/bronze/dolares/{path}', partition_cols, engine=\"fastparquet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Procesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtrar_convertir_limpiar(df):\n",
    "    columnas_deseadas = ['casa', 'moneda', 'compra', 'venta', 'fecha', 'hora']\n",
    "    \n",
    "    # Hacer una copia del DataFrame para evitar SettingWithCopyWarning\n",
    "    df_filtrado = df[columnas_deseadas].copy()\n",
    "   \n",
    "    # Reemplazar NaN por 0.00 en las columnas 'compra' y 'venta'\n",
    "    df_filtrado['compra'].fillna(0.00, inplace=True)\n",
    "    df_filtrado['venta'].fillna(0.00, inplace=True)\n",
    "    \n",
    "   \n",
    "    df_filtrado['fecha'] = pd.to_datetime(df_filtrado['fecha'])\n",
    "    df_filtrado['casa'] = df_filtrado['casa'].astype('category')\n",
    "    \n",
    "     # Agregar una columna 'id' con valores únicos para cada registro\n",
    "    df_filtrado['id'] = range(1, len(df_filtrado) + 1)\n",
    "     # Reorganizar columnas para colocar 'id' al principio\n",
    "    column_order = ['id'] + columnas_deseadas\n",
    "    df_filtrado = df_filtrado[column_order]\n",
    "    \n",
    "    return df_filtrado\n",
    "\n",
    "# Se leen los arqchivos parquet bronze\n",
    "tipos_cambio_dolar_parquet = pd.read_parquet('datalake/bronze/dolares/tipos_cambio_dolar') \n",
    "dolar_oficial_parquet = pd.read_parquet('datalake/bronze/dolares/dolar_oficial')\n",
    "dolar_blue_parquet = pd.read_parquet('datalake/bronze/dolares/dolar_blue') \n",
    "\n",
    "# Directorios y nombres de archivo\n",
    "directorios = ['dolar_oficial', 'dolar_blue', 'tipos_cambio_dolar']\n",
    "dfs_parquet = [dolar_oficial_parquet, dolar_blue_parquet, tipos_cambio_dolar_parquet]\n",
    "\n",
    "# Iterar sobre los DataFrames, nombres de archivo y  guarda el dataframe en la capa silver\n",
    "for df, path, tail_rows in zip(dfs_parquet, directorios, [1, 1, 7]):\n",
    "    df_resultado = filtrar_convertir_limpiar(df)\n",
    "    df_resultado = df_resultado.tail(tail_rows)\n",
    "    save(df_resultado, f'datalake/silver/dolares/{path}', partition_cols=None, engine=\"fastparquet\")\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Cargar las tablas a la base de datos de postgres\n",
    "#### SCD 1: La base de datos está diseñada para sobreescrbir la información "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear en postgres tabla dolares SCD Tipo 1\n",
    "\n",
    "eng = connect_to_postgres()\n",
    "with eng.begin() as con:\n",
    "    con.execute(text(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS leomarestrada_stg_dolares (\n",
    "            id INT PRIMARY KEY,\n",
    "            casa VARCHAR(50),\n",
    "            moneda VARCHAR(50),\n",
    "            compra DOUBLE PRECISION,\n",
    "            venta DOUBLE PRECISION,\n",
    "            fecha DATE,\n",
    "            hora INTEGER\n",
    "        );\n",
    "    \"\"\"))\n",
    "    \n",
    "    con.execute(text(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS leomarestrada_dolares_scd1 (\n",
    "            id INT PRIMARY KEY,\n",
    "            casa VARCHAR(50),\n",
    "            moneda VARCHAR(50),\n",
    "            compra DOUBLE PRECISION,\n",
    "            venta DOUBLE PRECISION,\n",
    "            fecha DATE,\n",
    "            hora INTEGER,\n",
    "            fecha_actualizacion_dwh TIMESTAMP\n",
    "        );\n",
    "    \"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tipos_cambio_dolar_parquet_silver = pd.read_parquet('datalake/silver/dolares/tipos_cambio_dolar') \n",
    "\n",
    "with eng.begin() as con:\n",
    "    con.execute(text(\"TRUNCATE TABLE leomarestrada_stg_dolares\"))\n",
    "\n",
    "    tipos_cambio_dolar_parquet_silver.to_sql(\"leomarestrada_stg_dolares\", con,\n",
    "                        if_exists=\"append\", method=\"multi\",\n",
    "                        index=False)\n",
    "\n",
    "    con.execute(text(\"\"\"\n",
    "        MERGE INTO leomarestrada_dolares_scd1\n",
    "        USING leomarestrada_stg_dolares AS dolares\n",
    "        ON (dolares.id = leomarestrada_dolares_scd1.id)\n",
    "        WHEN MATCHED THEN\n",
    "            UPDATE SET\n",
    "                casa = dolares.casa,\n",
    "                moneda = dolares.moneda,\n",
    "                compra = dolares.compra,\n",
    "                venta = dolares.venta,\n",
    "                fecha= dolares.fecha,\n",
    "                hora= dolares.hora,\n",
    "                fecha_actualizacion_dwh = CURRENT_TIMESTAMP\n",
    "        WHEN NOT MATCHED THEN\n",
    "            INSERT (id, casa, moneda, compra, venta, fecha, hora, fecha_actualizacion_dwh)\n",
    "            VALUES (\n",
    "                dolares.id,\n",
    "                dolares.casa,\n",
    "                dolares.moneda,\n",
    "                dolares.compra,\n",
    "                dolares.venta,\n",
    "                dolares.fecha,\n",
    "                dolares.hora,\n",
    "                CURRENT_TIMESTAMP\n",
    "            );\n",
    "    \"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SCD 2: La base de datos está diseñada para crear una columna historial de cambios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with eng.begin() as con:\n",
    "    con.execute(text(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS leomarestrada_dolar_blue_scd2 (\n",
    "            id_scd2 SERIAL PRIMARY KEY,\n",
    "            id INT,\n",
    "            casa VARCHAR(50),\n",
    "            moneda VARCHAR(50),\n",
    "            compra DOUBLE PRECISION,\n",
    "            venta DOUBLE PRECISION,\n",
    "            fecha_inicio DATE,\n",
    "            fecha_fin DATE,\n",
    "            es_actual BOOLEAN\n",
    "        )\n",
    "    \"\"\"))\n",
    "    \n",
    "with eng.begin() as con:\n",
    "    con.execute(text(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS leomarestrada_dolar_oficial_scd2 (\n",
    "            id_scd2 SERIAL PRIMARY KEY,\n",
    "            id INT,\n",
    "            casa VARCHAR(50),\n",
    "            moneda VARCHAR(50),\n",
    "            compra DOUBLE PRECISION,\n",
    "            venta DOUBLE PRECISION,\n",
    "            fecha_inicio DATE,\n",
    "            fecha_fin DATE,\n",
    "            es_actual BOOLEAN\n",
    "        )\n",
    "    \"\"\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dolar_blue_silver = pd.read_parquet('datalake/silver/dolares/dolar_blue') \n",
    "\n",
    "with eng.begin() as con:\n",
    "    con.execute(text(\"TRUNCATE TABLE leomarestrada_stg_dolares\"))\n",
    "\n",
    "    dolar_blue_silver.to_sql(\"leomarestrada_stg_dolares\", con,\n",
    "                              if_exists=\"append\", method=\"multi\",\n",
    "                              index=False)\n",
    "\n",
    "    con.execute(text(\"\"\"\n",
    "       BEGIN;\n",
    "\n",
    "       MERGE INTO leomarestrada_dolar_blue_scd2\n",
    "       USING leomarestrada_stg_dolares AS dolares\n",
    "       ON (dolares.id = leomarestrada_dolar_blue_scd2.id)\n",
    "       WHEN MATCHED AND dolares.venta <> leomarestrada_dolar_blue_scd2.venta AND leomarestrada_dolar_blue_scd2.es_actual = TRUE THEN\n",
    "            UPDATE SET\n",
    "                fecha_fin = CURRENT_DATE,\n",
    "                es_actual = FALSE\n",
    "       WHEN NOT MATCHED THEN\n",
    "            INSERT (id, casa, moneda, compra, venta, fecha_inicio, fecha_fin, es_actual)\n",
    "            VALUES (\n",
    "                dolares.id,\n",
    "                dolares.casa,\n",
    "                dolares.moneda,\n",
    "                dolares.compra,\n",
    "                dolares.venta,\n",
    "                dolares.fecha,\n",
    "                NULL,\n",
    "                TRUE\n",
    "            );\n",
    "\n",
    "        INSERT INTO leomarestrada_dolar_blue_scd2 (id, casa, moneda, compra, venta, fecha_inicio, es_actual)\n",
    "        SELECT s.id, s.casa, s.moneda, s.compra, s.venta, CURRENT_DATE, TRUE\n",
    "        FROM leomarestrada_stg_dolares s\n",
    "        LEFT JOIN leomarestrada_dolar_blue_scd2 c\n",
    "        ON s.id = c.id\n",
    "        WHERE s.venta <> c.venta AND c.es_actual = FALSE;\n",
    "\n",
    "        COMMIT;\n",
    "    \"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dolar_blue_silver = pd.read_parquet('datalake/silver/dolares/dolar_oficial') \n",
    "\n",
    "with eng.begin() as con:\n",
    "    con.execute(text(\"TRUNCATE TABLE leomarestrada_stg_dolares\"))\n",
    "\n",
    "    dolar_blue_silver.to_sql(\"leomarestrada_stg_dolares\", con,\n",
    "                              if_exists=\"append\", method=\"multi\",\n",
    "                              index=False)\n",
    "\n",
    "    con.execute(text(\"\"\"\n",
    "       BEGIN;\n",
    "\n",
    "       MERGE INTO leomarestrada_dolar_oficial_scd2\n",
    "       USING leomarestrada_stg_dolares AS dolares\n",
    "       ON (dolares.id = leomarestrada_dolar_oficial_scd2.id)\n",
    "       WHEN MATCHED AND dolares.venta <> leomarestrada_dolar_oficial_scd2.venta AND leomarestrada_dolar_oficial_scd2.es_actual = TRUE THEN\n",
    "            UPDATE SET\n",
    "                fecha_fin = CURRENT_DATE,\n",
    "                es_actual = FALSE\n",
    "       WHEN NOT MATCHED THEN\n",
    "            INSERT (id, casa, moneda, compra, venta, fecha_inicio, fecha_fin, es_actual)\n",
    "            VALUES (\n",
    "                dolares.id,\n",
    "                dolares.casa,\n",
    "                dolares.moneda,\n",
    "                dolares.compra,\n",
    "                dolares.venta,\n",
    "                dolares.fecha,\n",
    "                NULL,\n",
    "                TRUE\n",
    "            );\n",
    "\n",
    "        INSERT INTO leomarestrada_dolar_oficial_scd2 (id, casa, moneda, compra, venta, fecha_inicio, es_actual)\n",
    "        SELECT s.id, s.casa, s.moneda, s.compra, s.venta, CURRENT_DATE, TRUE\n",
    "        FROM leomarestrada_stg_dolares s\n",
    "        LEFT JOIN leomarestrada_dolar_oficial_scd2 c\n",
    "        ON s.id = c.id\n",
    "        WHERE s.venta <> c.venta AND c.es_actual = FALSE;\n",
    "\n",
    "        COMMIT;\n",
    "    \"\"\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
